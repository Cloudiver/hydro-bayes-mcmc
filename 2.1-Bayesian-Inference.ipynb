{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics in Model Performance\n",
    "\n",
    "开始正式的贝叶斯推断介绍前，先了解一些基本概念。\n",
    "\n",
    "参考资料：\n",
    "\n",
    "- Bayesian Analysis with Python: Introduction to Statistical Modeling and Probabilistic Programming with PyMC3 and ArViz, 2nd Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能听说过这句话\n",
    "\n",
    "`所有模型都是错误的，但有些是有用的`\n",
    "\n",
    "这句格言归功于英国统计学家 George E.P.Box。这意味着模型应该代表现实，但它们很少完全保真。然而，一些模型可以提供有用的洞察力，而另一些模型对我们已经知道的东西几乎没有什么价值，或者更糟糕的是，它为我们提供了误导性的信息。\n",
    "\n",
    "那么我们如何确定我们的模型是否可信呢？在机器学习中，我们采用交叉验证来为我们提供一种关于模型泛化到看不见的数据的能力的衡量。现在让我们看看其他一些以信息论为基础的衡量标准，以评估我们模型的预测能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting vs. Overfitting\n",
    "\n",
    "大多数人现在都听说过欠拟合和过拟合模型。更简单的模型应该是首选，但不能以牺牲准确性为代价。另一方面，过拟合模型可能无法很好地概括新数据。我们可以使用衡量解释方差比例的 $R^2$ 指标来衡量模型对数据的拟合程度。\n",
    "\n",
    "如果我们使用线性回归的例子并从一阶多项式回归开始解释数据，我们可能会发现数据可能没有被充分捕获。这被称为欠拟合。我们可能不得不通过增加多项式的阶来逐步增加模型的复杂性。然而，过了某个点，模型开始过度拟合数据。这意味着该模型只是使用其表示能力来记忆数据，并且在输入模型的新数据上表现不佳（\\\\(R^2\\\\) 太高）。\n",
    "\n",
    "我们想要一个在欠拟合和过拟合之间找到平衡的模型，这种权衡通常被称为偏差-方差权衡。偏差是由于无法容纳数据而导致的数据错误。该模型没有捕获数据中所有变化和模式的表示能力。方差是模型对数据的敏感性导致的误差，通常是模型过于复杂造成的。由于这个原因，经常使用正则化通过最小化系数的数量来降低回归模型（或神经网络）的复杂性。\n",
    "\n",
    "![Underfitting vs. Overfitting (from AWS)](https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png)\n",
    "\n",
    "<center>Underfitting vs Overfitting (from AWS  docs)</center>\n",
    "\n",
    "## $R^2$ and Explained Variance\n",
    "\n",
    "### What does it do?\n",
    "\n",
    "$R^2$ 是一种拟合优度度量，它告诉您数据与我们创建的模型的拟合程度。它解释了自变量解释的结果中方差的比例。\n",
    "\n",
    "### Derivation\n",
    "\n",
    "如果我们观察 $y_i$ 给出的数据，使得拟合模型为每个点 i 预测 $f_i$，我们可以写出所有观察数据的平均值，由 \\\\(y_{mean}\\\\) 给出\n",
    "\n",
    "$$y_{mean} = \\dfrac{1}{n} \\sum_i y_i$$\n",
    "\n",
    "* 与数据方差成正比的总平方和为\n",
    "\n",
    "$$SS_{tot} = \\sum_i (y_i - y_{mean})^2$$\n",
    "\n",
    "* 残差平方和（也称为误差）定义为\n",
    "\n",
    "$$SS_{res} = \\sum_i (y_i - f_i)^2$$\n",
    "\n",
    "* 现在，\\\\(R^2\\\\) 被定义为\n",
    "\n",
    "$$R^2 = 1 - \\dfrac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "### How do we interpret this?\n",
    "\n",
    "* \\\\(R^2\\\\) 对于完全符合观察数据的模型为 1，即 \\\\(f_i = y_i\\\\) 对所有 i。\n",
    "\n",
    "* 如果模型预测 \\\\(y_{mean}\\\\) 总是那么 \\\\(SS_{res} = SS_{tot}\\\\) 和 \\\\(R^2 = 0\\\\)，这表示基线模型所有其他模型都可以与之进行比较。\n",
    "\n",
    "* 任何表现比基线模型差的模型都会有一个负的 $R^2$ 分数。\n",
    "\n",
    "### Explained Variance \n",
    "\n",
    "术语 $\\dfrac{SS_{res}}{SS_{tot}}$ 也称为无法解释的方差。所以\n",
    "\n",
    "$R^2 = 1 - Unexplained Variance$\n",
    "\n",
    "or \n",
    "\n",
    "$R^2 = Explained Variance$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures for Predictive Performance\n",
    "\n",
    "模型的精度可以通过以下方法来衡量：\n",
    "\n",
    "### 1. Cross-Validation \n",
    "\n",
    "在这里，我们将数据划分为不重叠的子集，并对不同的子集进行训练和验证。根据我们执行此交叉验证的方式，它可以称为 K 折交叉验证或留一法交叉验证 (LOOCV)。在 **K-fold Cross-Validation** 中，我们将数据划分为“K”个折叠或子集，在 k-1 折叠上执行模型训练，同时在剩下的 1 折叠上评估模型性能。我们迭代地选择每个折叠作为测试折叠，而其他折叠成为训练折叠。\n",
    "\n",
    "\n",
    "![Image from the scikit-learn page for K-fold cross validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png) \n",
    "\n",
    "\n",
    "<center><i>K-fold Cross-Validation from the scikit-learn page</i></center>\n",
    "\n",
    "如果折叠数等于数据点数，我们有**留一法交叉验证**。\n",
    "\n",
    "### 2. Information criteria\n",
    "\n",
    "**Reference** \n",
    "\n",
    "* [Predictive metrics presentation from Liberty Mutual](https://www.casact.org/education/rpm/2016/presentations/PM-LM-4-Tevet.pdf)\n",
    "\n",
    "许多牢牢植根于信息论的思想帮助我们量化模型的表现。\n",
    "\n",
    "1. Log-likelihood (Log predictive density) and deviance\n",
    "\n",
    "2. Akaike Information Criterion (AIC)\n",
    "\n",
    "3. Widely Applicable Information Criterion (WAIC)\n",
    "\n",
    "4. Deviance Information Criterion (DIC) \n",
    "\n",
    "5. Bayesian Information Criterion (BIC)\n",
    "\n",
    "对 (2) 到 (5),\n",
    "\n",
    "* 它们采用方程的形式，其中有两个项由下式给出\n",
    "\n",
    "    $$metric = model\\;fit + penalization$$\n",
    "    \n",
    "* 模型拟合是使用给定模型参数的数据的对数似然来测量的（可以是逐点估计，也可以使用完整的后验分布）\n",
    "\n",
    "* 较低的值意味着更好的拟合\n",
    "\n",
    "AIC、BIC 和 DIC 使用数据的联合概率，而 WAIC 计算数据的逐点概率。下面，我们假设模型参数是独立的，因此联合概率与逐点估计的乘积相同。\n",
    "\n",
    "### Log-likelihood and Deviance\n",
    "\n",
    "**Reference**\n",
    "\n",
    "* [Cousineau, Denis et Teresa A. Allan. \"Likelihood and its use in Parameter Estimation and Model Comparison.\" Mesure et évaluation en éducation, volume 37, number 3, 2015, p. 63–98. https://doi.org/10.7202/1036328ar](https://www.erudit.org/en/journals/mee/2015-v37-n3-mee02497/1036328ar/)\n",
    "\n",
    "这些术语用于衡量我们模型中与模型试图拟合的数据有关的误差。大多数人都熟悉由下式给出的均方误差 (MSE)\n",
    "\n",
    "$$MSE = \\sum_1^n (y_{true} - y_{predicted})^2 / n$$\n",
    "\n",
    "#### Log-likelihood (Log predictive density)\n",
    "\n",
    "虽然这是一种完全可以接受的测量误差的方法，尤其是在似然是正态分布的情况下，但在理论上更合理的测量模型性能的方法是使用对数似然函数。\n",
    "\n",
    "$$Loglikelihood = \\sum_1^n log p(y_i | \\theta)$$\n",
    "\n",
    "\n",
    "如果似然函数为正态，则对数似然与 MSE 成正比。\n",
    "\n",
    "#### Deviance\n",
    "\n",
    "偏差是从饱和模型的对数似然中减去模型的对数似然的两倍。饱和模型是一种过度拟合的模型，它完美地拟合了观察到的数据。可以重写以强调值的范围现在是从 0 到 \\\\(\\infty\\\\)。\n",
    "\n",
    "$$Deviance = -2 \\sum_1^n ( log p(y_i | \\theta) - log p(y_i | \\theta_s) $$\n",
    "\n",
    "#### Why use the Deviance over the Log-likelihood?\n",
    "\n",
    "请注意，似然函数 $p(y_i | \\theta)$ 的值从 0 表示不拟合到 1 表示完全拟合模型。这导致对数似然函数采用从 $- \\infty$ 到 0 的值。将对数似然函数乘以 -2 会得到一个类似于 MSE 的可解释数字。\n",
    "\n",
    "* 拟合不佳的模型具有较大的正值\n",
    "\n",
    "* 完美拟合模型的值为 0。\n",
    "\n",
    "复杂模型在训练集（样本内数据）上的偏差值较低，这需要在比较模型时进行惩罚。这与我们之前谈到的模型过度拟合有关。\n",
    "\n",
    "#### A Note on MLE\n",
    "\n",
    "最大似然估计 (MLE) 基于估计参数 $\\theta$ 的概念，该参数使概率 $\\sum_1^n p(y_i | \\theta)$ 最大化。虽然还有其他方法可以做到这一点，但如果样本量足够大，MLE 是分布参数 $\\theta$ 的最有效估计器。此外，随着样本量的增加，估计参数趋于真实参数，误差变为正态分布。\n",
    "\n",
    "当您有非正态分布（即参数受观察值约束的分布）时，MLE 的缺点就会出现。对于此类分布，可能不存在最大似然。在存在多个最大值的情况下，可能会出现类似的问题。\n",
    "\n",
    "### Posterior Predictive Distribution to Estimate Predictive Accuracy\n",
    "\n",
    "这就给了利用**后验预测分布**的场景，这是贝叶斯的。这使我们能够测量模型生成新数据的概率，即 \\\\(p(y_{new} | y)\\\\)。这可以解释为问“鉴于在样本内数据上训练的模型，看到新的样本外数据的概率是多少？”。预测准确率可以写为\n",
    "\n",
    "$$accuracy = p(y_{new} | y) = \\int p(y_{new} | \\theta) p(\\theta | y) d \\theta$$\n",
    "\n",
    "这么做的原因是参数现在不是一个点估计，而是一个分布，我们需要计算所有可能性。\n",
    "\n",
    "其中 \\\\(p(\\theta | y)\\\\) 是 \\\\(\\theta\\\\) 的后验分布，我们对 \\\\(\\theta\\\\) 的整个分布进行积分。现在这就是 \\\\(p(y_{new} | \\theta)\\\\) 对 \\\\(\\theta\\\\) 后验分布的期望。简单来说，它是所有看到 \\\\(y_{new}\\\\) 的概率在 \\\\(\\theta\\\\) 的所有可能值上计算的平均值。\n",
    "\n",
    "$$accuracy = E [ p(y_{new} | \\theta) ]$$\n",
    "\n",
    "这需要以下步骤\n",
    "\n",
    "1. 从 \\\\(\\theta\\\\) 的后验分布中抽样一个 \\\\(\\theta_i\\\\)\n",
    "\n",
    "2. 给定 \\\\(\\theta_i\\\\) 的值，计算我们能看到 \\\\(y_{new}\\\\)  的可能性有多大，也就是计算 \\\\(p(y_{new} | \\theta_i)\\\\)\n",
    "\n",
    "3. 重复（1）和（2）多次以计算 \\\\(p(y_{new} | \\theta)\\\\) 的期望\n",
    "\n",
    "经常使用log计算：\n",
    "\n",
    "$$accuracy = log( E [ p(y_{new} | \\theta) ])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike Information Criterion (AIC)\n",
    "\n",
    "AIC 源自频率统计者世界，不使用后验分布。因此，它不是对后验积分，而是使用 \\\\(\\theta\\\\) 的 MLE 估计。\\\\(E [ p(y_{new} | \\theta) ]\\\\) 现在替换为 \\\\(p(y_{new} | \\theta_{mle})\\\\)。\n",
    "\n",
    "$$AIC = -2 \\sum_{i=1}^n log p(y_i | \\theta_{mle}) + 2 n_{parameters}$$\n",
    "\n",
    "这里 \\\\(n_{parameters}\\\\) 是指模型中的参数数量，\\\\(\\theta_{mle}\\\\) 是 \\\\(\\theta\\\\) 的 MLE 估计。我们想要一个具有较低 AIC 的模型，第二项旨在通过增加 AIC 的值来惩罚复杂模型。由于这不使用后验分布，因此不考虑有关参数不确定性的任何信息。\n",
    "\n",
    "### Bayesian Information Criterion (BIC)\n",
    "\n",
    "BIC 与 AIC 非常相似（实际上根本不是贝叶斯）。第一项与 AIC 中的相同，但偏差校正项现在也包含样本数。\n",
    "\n",
    "$$BIC = -2 \\sum_{i=1}^n log p(y_i | \\theta_{mle}) + n_{parameters} \\; log \\; n_{samples}$$\n",
    "\n",
    "### Deviance Information Criterion (DIC)\n",
    "\n",
    "DIC 是一种更贝叶斯的替代方法，它使用后验平均点估计 \\\\(\\theta_{Bayes}\\\\) 而不是 MLE 估计。这里 \\\\(\\theta_{Bayes}\\\\) 是 \\\\(\\theta\\\\) 的期望值。\n",
    "\n",
    "$$DIC = -2 \\sum_{i=1}^n log p(y_i | \\theta_{Bayes}) + 2 var_{posterior} \\; log p(y_i | \\theta )$$\n",
    "\n",
    "\n",
    "### Widely Applicable Information Criterion (WAIC)\n",
    "\n",
    "**Reference**\n",
    "\n",
    "* [WAIC by Gelman](http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf)\n",
    "\n",
    "广泛适用的信息准则或 WAIC 是 AIC 的贝叶斯扩展。对数逐点预测密度的推导类似于我们上面介绍的内容，但在此处进行复制以使其与引用的论文保持一致。\n",
    "\n",
    "#### Log pointwise predictive density (lppd)\n",
    "\n",
    "可以定义新数据点 $y_{new}$ 的预测值\n",
    "\n",
    "$$p_{post}(y_{new}) = \\int p(y_{new} | \\theta) p_{post}(\\theta) d \\theta $$\n",
    "\n",
    "如果我们取双方的对数，我们得到\n",
    "\n",
    "$$log p_{post}(y_{new}) = log \\int p(y_{new} | \\theta) p_{post}(\\theta) d \\theta$$\n",
    "\n",
    "其中 \\\\(p_{post}(\\theta)\\\\) 是通过训练我们的模型获得的 \\\\(\\theta\\\\) 的后验分布。这是新点的预测拟合。如果我们有许多新数据点 i=1,...n 我们可以为使用新数据的模型的对数逐点预测密度编写以下内容\n",
    "\n",
    "$$lppd = log \\prod_i p_{post} (y_{new_i}) = \\sum_i \\int log p(y_{new_i} | \\theta ) p_{post} (\\theta) d \\theta$$\n",
    "\n",
    "* 实际上，\\\\(\\theta\\\\) 上的内积分是使用\\\\(\\theta\\\\)（采样）的可能值的平均值计算的，表示为\\\\(\\theta_S\\\\)。\n",
    "\n",
    "$$\\sum_i \\int log p(y_{new_i} | \\theta ) p_{post} (\\theta) d \\theta = \\sum_i log \\dfrac{1}{S} \\sum_S p(y_{new_i} | \\theta_{S})$$\n",
    "\n",
    "* 现在假设我们没有保持集 $y_{new}$ 并且我们在我们的训练集上计算 lppd，这不是衡量模型未来性能的好方法。因此 WAIC 添加了一个术语来纠正这种高估的性能。此校正测量 y 的每个元素在 \\\\(\\theta_S\\\\) 的不同样本上计算的对数似然方差。这种校正可以看作是一种旨在减少参数数量的惩罚，因为更多的模型参数意味着更大的后验分布或方差。\n",
    "\n",
    "$$2 \\cdot \\sum_i Var_{s} ( log p(y_{new_i} | \\theta_{S}) )$$\n",
    "\n",
    "* WAIC 现在定义为上述两项的总和\n",
    "\n",
    "$$WAIC = -2 \\sum_i log \\dfrac{1}{S} \\sum_S p(y_{new_i} | \\theta_{S}) +  2 \\sum_i Var_{s} ( log p(y_{new_i} | \\theta_{S}) )$$\n",
    "\n",
    "### A Qualitative Discussion\n",
    "\n",
    "* AIC 可能不适用于更复杂的模型，因为这只是使用参数数量来惩罚模型\n",
    "* 这里值得强调的是，除了交叉验证（带有测试集）之外，上述所有指标/方法都**使用样本内数据来评估样本外性能**。这类似于使用训练集而不是测试集来评估机器学习中的模型性能。然而，与机器学习中通常执行的不同，我们应用偏差校正来纠正通过估计样本数据的性能而引入的错误。但是，如果使用测试集，交叉验证就完全不需要这种纠错。\n",
    "\n",
    "* AIC 和 DIC 更容易计算，但与 WAIC 不同，它们不是完全贝叶斯的。但是，WAIC 的计算量更大。\n",
    "\n",
    "* 可以通过计算对数后验密度使交叉验证成为贝叶斯。然而，与其他技术相比，这在计算上变得相当昂贵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and KL Divergence\n",
    "\n",
    "**Reference**\n",
    "\n",
    "* [Information Theory](https://arxiv.org/pdf/1511.00860.pdf)\n",
    "\n",
    "在下面的示例中使用了求和，因为假设离散分布。连续分布的话可以用积分代替。\n",
    "\n",
    "### Entropy\n",
    "\n",
    "如果存在概率分布由 P(x) 给出的随机离散变量“x”，则随机变量“x”的熵是信息不确定性的度量，可以计算为 \n",
    "\n",
    "$$H(x) = - \\sum_x \\: p(x) log \\:  p(x) $$\n",
    "\n",
    "其中较大的熵值表示较高的不确定性。从几何上讲，我们可以将其可视化为具有更大分布的分布。虽然很容易将其与方差等同起来，但有一些例子，例如双峰高斯分布，其中增加方差不会增加熵。熵是一个点周围概率密度质量的度量，而方差衡量概率质量从平均值延伸的程度。可能有两个相距很远的窄模式，由于值的相对确定性（围绕窄模式），这表示高方差但低熵。\n",
    "\n",
    "熵是定义先验的有用方法，因为具有高熵的先验可以用作无信息先验。给定参数值的某些限制，以下可以用作这些参数的先验。\n",
    "\n",
    "1. 无约束——均匀分布\n",
    "2. 高密度区域的正均值 - 指数分布\n",
    "3. 固定方差 - 正态分布\n",
    "4. 具有固定均值的两个结果 - 二项分布\n",
    "\n",
    "下面的代码展示了不同分布的熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb442937df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example from [1]\n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(912)\n",
    "x = range(0, 10)\n",
    "q = stats.binom(10, 0.75)\n",
    "q2 = stats.binom(10, 0.5)\n",
    "r = stats.randint(0, 10)\n",
    "true_distribution = [list(q.rvs(200)).count(i) / 200 for i in x]\n",
    "q_pmf = q.pmf(x)\n",
    "q2_pmf = q2.pmf(x)\n",
    "r_pmf = r.pmf(x)\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "ax[0].vlines(x, 0, q_pmf, label=f'entropy = {stats.entropy(q_pmf):.2f}')\n",
    "ax[1].vlines(x, 0, r_pmf, label=f'entropy = {stats.entropy(r_pmf):.2f}')\n",
    "ax[2].vlines(x, 0, q2_pmf, label=f'entropy = {stats.entropy(q2_pmf):.2f}')\n",
    "ax[0].set_title('Binomial distribution with parameter 0.75')\n",
    "ax[1].set_title('Random distribution')\n",
    "ax[2].set_title('Binomial disttribution with parameter 0.5')\n",
    "fig.suptitle(\"Entropy of different distributions\")\n",
    "stats.entropy(true_distribution)\n",
    "#ax[idx].set_xticks(x)\n",
    "ax[0].legend(loc=2, handlelength=0)\n",
    "ax[1].legend(loc=2, handlelength=0)\n",
    "ax[2].legend(loc=2, handlelength=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence\n",
    "\n",
    "KL Divergence 类似于熵的概念，只是它用于比较两个分布的相似度和接近度。它被定义为两个离散分布\n",
    "\n",
    "$$KL(p||q) = \\sum_x p(x) \\: log \\dfrac{p(x)}{q(x)}$$\n",
    "\n",
    "KL 散度的值从相同分布的 0 变化到无穷大，具体取决于分布之间的差异。重要的是要理解这不是距离度量，因为它不是对称的，因为\n",
    "\n",
    "$$KL(q||p) = \\sum_x q(x) \\: log \\dfrac{q(x)}{p(x)}$$\n",
    "\n",
    "Jensen Shannon 散度是 KL 散度的对称版本\n",
    "\n",
    "KL 散度 $KL(p||q)$ 可以看作是两个熵的差异\n",
    "\n",
    "$$KL(p||q) = \\sum_x p(x) \\: log p(x) - \\sum_x p(x) \\: log q(x)$$\n",
    "\n",
    "其中第一项是“p”的熵，第二项是“p”和“q”之间的交叉熵。\n",
    "\n",
    "KL 散度通常用于机器学习中来学习分布。如果真实分布可用，则可以优化提议的分布以使其尽可能接近真实分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence between the true distribution and the uniform distribution  0.7394593875511319\n",
      "KL Divergence between the true distribution and the q distribution  0.009657896086383405\n",
      "KL Divergence between the true distribution and the q2 distribution  1.276465607901914\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "print(\"KL Divergence between the true distribution and the uniform distribution \",entropy(true_distribution, r_pmf))\n",
    "print(\"KL Divergence between the true distribution and the q distribution \",entropy(true_distribution, q_pmf))\n",
    "print(\"KL Divergence between the true distribution and the q2 distribution \",entropy(true_distribution, q2_pmf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging\n",
    "\n",
    "有时，模型选择可能不是最适合我们问题的解决方案，例如当没有单个模型提供令人满意的性能时。或者，如下文所述，当我们想要利用与不同模型相关的方差作为不确定性的度量时。在这些情况下，我们可能想求助于模型平均。\n",
    "\n",
    "### Pseudo Bayesian Modeling Averaging\n",
    "\n",
    "[Using Stacking to Average Bayesian Predictive Distributions](http://www.stat.columbia.edu/~gelman/research/published/stacking_paper_discussion_rejoinder.pdf)\n",
    "\n",
    "当有多种模型可供选择时，很容易选择具有最佳性能的模型（取决于我们如何定义性能）。然而，这样做我们忽略了其他模型提供的不确定性信息。减轻这种不确定性的一种方法是执行模型平均。通过使用所有模型的加权平均获得的元模型可用于进行预测。完成这种平均的一种方法是计算类似于使用 softmax 公式的权重\n",
    "\n",
    "$$w_i = \\dfrac{e^{-dE_i / 2}}{\\sum_j e^{-dE_j / 2}}$$\n",
    "\n",
    "其中 \\\\(dE_i\\\\) 是第 i 个模型的 WAIC 值与具有最低 WAIC 的模型相比的差异。\n",
    "\n",
    "任何信息标准度量都可以在这个等式中使用，例如 AIC。使用以这种方式计算的权重对模型求平均称为伪贝叶斯建模平均。\n",
    "\n",
    "### Stacking\n",
    "\n",
    "最近提出的另一种技术是预测分布的堆叠。这背后的想法是组合模型，以便最小化加权元模型和真实模型之间的差异。当使用对数分数时，类似于 KL 散度，可以使用以下等式\n",
    "\n",
    "$$model = max_w \\dfrac{1}{n} \\sum_i^{n} log \\sum_k w_k p(y_i | y_{-i}, M_k)$$\n",
    "\n",
    "其中 n 是数据点的数量，\\\\(M_k\\\\) 是第 k 个模型，\\\\(w_k\\\\) 是应用于第 k 个模型的权重。 \\\\(y_{-i}\\\\) 是 y 中除 \\\\(y_i\\\\) 之外的每个元素。术语 \\\\(p(y_i | y_{-i}, M_k)\\\\) 对应于使用留一法交叉验证 (LOOCV) 程序的预测概率密度。目标是选择使看到 \\\\(y_i\\\\) 的概率最大化的权重组合，从而为我们提供理想的元模型，据我们所知，基于可用数据，最大限度地减少分歧。请注意，这里的 argmax 是在 'w' 上计算的，而不是在某些资源中列出的 'n'。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
