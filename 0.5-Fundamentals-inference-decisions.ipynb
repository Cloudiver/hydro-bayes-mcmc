{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Decisions\n",
    "\n",
    "我们一直在谈论概率，并且有一些工具可以用来操纵分布和估计参数。我们如何将其转化为决策？在这里，我们将讨论一些可能有助于为该决定提供信息的措施。\n",
    "\n",
    "<hr style=\"border:2px solid blue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "在上一节中，我们举例说明了确定疾病状态的测试。我们接受了测试并发现了一个阳性结果，这导致我们真正被感染的可能性为 79%。很有可能，我们患有这种疾病。我们是否足够确定我们会寻求适当的治疗，或者我们可能需要第二意见？有第二次测试吗？我们是否简单地进行两次测试并将结果结合起来？当我们建立一个计算模型时，我们会反复问这个问题，并且必须做出决定，我们是继续当前的方向，还是从我们来的地方走。做出错误选择的后果往往是不对称的。我们的总体目标不只是避免错误，而是要考虑决策及犯错的后果。\n",
    "\n",
    "在概率方面表达这一点，就是 对于二元决策，我们的目标是：\n",
    "\n",
    "$$\\text{argmin } p(mistake) = \\sum_{i=1}^k p(x_{k \\notin j},C_k)$$ 其中 j/k 是类别标签\n",
    "\n",
    "## Rejection regions or decision boundaries\n",
    "\n",
    "在我们的决定中，我们可能会选择一个区域包含在一个或另一个类别中，例如，我们可能决定疾病分类中的最佳边界为 $p(disease|+test)>75%$。请注意，决策边界应划分空间。带决策边界表示：\n",
    "\n",
    "$$\\text{argmin } p(mistake) = \\int_{\\mathbb{R_1}} p(x,C_2) dx + \\int_{\\mathbb{R_2}} p(x,C_1) dx$$\n",
    "\n",
    "\n",
    "回到抛硬币实验，试验一个硬币掷1000次，执行1000次试验，然后统计每次试验里正面朝上的比例，然后绘图如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds are:\n",
      "[0.4, 0.65]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEGCAYAAAB4lx7eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPzklEQVR4nO3df5BdZX3H8c+n2aQSk4g12842EBedNm3KaJDFkCJOiswUomNsyzQaKsU6jY6txlattp3aEZ1pbWc6DqUWIjKpVgRbkEQG6+CPGCghsIEN5od0aBQK3Wk2rWUJOqGBb/84Z8k17OaevXvPvfu9+37N3Mm5e84993ue3P3ss2fPeR5HhAAAef1EtwsAAMwMQQ4AyRHkAJAcQQ4AyRHkAJBcXx07Xbp0aQwODtaxa2Q0/nDx75IV3a2jV9CePWnPnj1HIqK/ldfWEuSDg4MaHh6uY9fI6Otri38v3tHNKnoH7dmTbD/a6ms5tQIAyRHkAJAcQQ4AyRHkAJAcQQ4AyRHkAJAcQQ4AydVyHTkwG924+zFtG3mi22Vo/apl2rh6ebfLQA+hR445Y9vIEzowOt7VGg6Mjs+KHyboLfTIMaesHFiim9+1pmvvv+G6XV17b/QueuQAkBxBDgDJEeQAkBxBDgDJEeQAkBxBDgDJEeQAkBxBDgDJEeQAkBxBDgDJEeQAkBxBDgDJEeQAkFzlILc9z/aDtm+vsyAAwPRMp0e+WdLBugoBALSmUpDbPkPSGyVdX285AIDpqtoj/5SkP5L03FQb2N5ke9j28NjYWDtqAwBU0DTIbb9J0uGI2HOq7SJiS0QMRcRQf39/2woEAJxalR75BZLebPv7km6SdJHtf6y1KgBAZU2DPCL+OCLOiIhBSW+V9M2I+K3aKwMAVMJ15ACQXN90No6IHZJ21FIJAKAl9MgBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSm9bky0Ar/mv8mI4cPaarrtvV1ToOjI5r5cCSrtYA1IEeOWp35Ogx/fCZ490uQysHlmj9qmXdLgNoO3rk6IiFC/p08xVrul0G0JPokQNAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAck2D3PaLbN9ne6/t/bY/1onCAADVVBnG9pikiyLiqO35ku62/dWIuLfm2gAAFTQN8ogISUfLp/PLR9RZFACgukrnyG3Psz0i6bCkOyNi9yTbbLI9bHt4bGyszWUCAKZSKcgj4tmIWCXpDEmvtX32JNtsiYihiBjq7+9vc5kAgKlM66qViPhfSTskXVJHMQCA6aty1Uq/7dPL5dMkXSzpuzXXBQCoqMpVKwOS/sH2PBXB/6WIuL3esgAAVVW5auUhSed0oBYAQAu4sxMAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASC5pkFu+0zb37J90PZ+25s7URgAoJq+Ctscl/SBiHjA9mJJe2zfGREHaq4NAFBB0x55RIxGxAPl8lOSDkpaVndhAIBqpnWO3PagpHMk7Z5k3Sbbw7aHx8bG2lQeAKCZykFue5GkWyS9PyLGT14fEVsiYigihvr7+9tZIwDgFCoFue35KkL8CxFxa70lAQCmo8pVK5b0WUkHI+Jv6i8JADAdVXrkF0h6u6SLbI+Uj3U11wUAqKjp5YcRcbckd6AWAEALuLMTAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEgOYIcAJIjyAEguaZBbvsG24dt7+tEQQCA6anSI98q6ZKa6wAAtKiv2QYRsdP2YAdqAeaEA6Pj2nDdrpZf/9HTxiVJV81gH5K0ftUybVy9fEb7wOzQNMirsr1J0iZJWr6cDwcwmfWrlnW7BEnFDxNJBHmPaFuQR8QWSVskaWhoKNq1X6CXbFy9fObh+fUlkqSbL17T8i5m8hsBZh+uWgGA5AhyAEiuyuWHX5S0S9IK24/bfmf9ZQEAqqpy1crbOlEIAKA1nFoBgOQIcgBIjiAHgOQIcgBIjiAHgOQIcgBIrm236GN2unH3Y9o28kRXa/jgvONauICPGlAXeuQ9btvIE88PkNQtCxf0aemin+xqDUAvo5s0B6wcWKKb39X6AEszVg7yBKAe9MgBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDmCHACSI8gBIDnGI6/RbJid58DouFYOMB440MvokddoNszOs3JgidavWtbVGgDUix55zbo+Ow+AnkePHACSI8gBIDlOrQBz1IHRcW24bldXa1i/apk2rl7e1Rp6AUEOzEGz4Q/gExcCEOQzR5ADc9DG1cu7HqDd/m2gl3COHACSI8gBIDmCHACSI8gBILlKQW77EtsP237E9kfqLgoAUF3TILc9T9LfSbpU0kpJb7O9su7CAADVVLn88LWSHomIQ5Jk+yZJ6yUdmOoFh8ae5tIiMfIg0MxsuCmpF1QJ8mWS/qPh+eOSVp+8ke1NkjZJ0qKBV7aluOwYeRCYGt8b7VMlyD3J1+IFX4jYImmLJA0NDQUj/gE4ldlwU9Js8qV3t/7aKn/sfFzSmQ3Pz5D0n62/JQCgnaoE+f2Sfs72WbYXSHqrpO31lgUAqKrpqZWIOG779yV9TdI8STdExP7aKwMAVFJp0KyIuEPSHTXXAgBoAXd2AkByBDkAJEeQA0ByBDkAJOeIF9zbM/Od2k9JerjtO85pqaQj3S5iFqAdTqAtTqAtTlgREYtbeWFdU709HBFDNe07FdvDtAXt0Ii2OIG2OMH2cKuv5dQKACRHkANAcnUF+Zaa9psRbVGgHU6gLU6gLU5ouS1q+WMnAKBzOLUCAMkR5ACQXMtB3mxCZheuLtc/ZPs1Myt19qrQFpeXbfCQ7Xtsv7obdXZC1Ym6bZ9n+1nbl3Wyvk6q0ha219oesb3f9rc7XWOnVPgeeYntr9jeW7bFO7pRZ91s32D7sO19U6xvLTcjYtoPFcPZ/rukV0haIGmvpJUnbbNO0ldVzDB0vqTdrbzXbH9UbItflvTScvnSudwWDdt9U8WImpd1u+4ufi5OVzH37fLy+U93u+4utsWfSPpkudwv6X8kLeh27TW0xeslvUbSvinWt5SbrfbIn5+QOSKekTQxIXOj9ZI+F4V7JZ1ue6DF95vNmrZFRNwTET8on96rYpalXlTlcyFJ75V0i6TDnSyuw6q0xUZJt0bEY5IUEb3aHlXaIiQttm1Ji1QE+fHOllm/iNip4tim0lJuthrkk03IfPJMqlW26QXTPc53qviJ24uatoXtZZJ+TdK1HayrG6p8Ln5e0ktt77C9x/YVHauus6q0xTWSflHFNJLfkbQ5Ip7rTHmzSku52eot+lUmZK40aXMPqHyctn9FRZC/rtaKuqdKW3xK0ocj4tmi89WzqrRFn6RzJb1B0mmSdtm+NyL+re7iOqxKW/yqpBFJF0l6paQ7bd8VEeM11zbbtJSbrQZ5lQmZ58qkzZWO0/arJF0v6dKI+O8O1dZpVdpiSNJNZYgvlbTO9vGIuK0jFXZO1e+RIxHxtKSnbe+U9GpJvRbkVdriHZL+MooTxY/Y/p6kX5B0X2dKnDVays1WT61UmZB5u6Qryr/Cni/pyYgYbfH9ZrOmbWF7uaRbJb29B3tbjZq2RUScFRGDETEo6Z8lvacHQ1yq9j2yTdKFtvtsL5S0WtLBDtfZCVXa4jEVv5nI9s9IWiHpUEernB1ays2WeuQxxYTMtt9drr9WxRUJ6yQ9IumHKn7i9pyKbfFRSS+T9OmyJ3o8enDEt4ptMSdUaYuIOGj7XyQ9JOk5SddHxKSXpWVW8XPxcUlbbX9HxemFD0dEzw1va/uLktZKWmr7cUl/Lmm+NLPc5BZ9AEiOOzsBIDmCHACSI8gBIDmCHACSI8gBIDmCHDNWjmI40vAYnOH+Vtle1/D8zacaSbEdbL/P9kHbXzjp62tt337S17a2a9RG20fbsR/Mba3e2Qk0+lFErJpsRTkIkqc5bsYqFXeA3iFJEbFdL7yBpN3eo+Ku2+/V/D5A29EjR9vZHix7t5+W9ICkM23/ve3hcqzpjzVse145Rvte2/fZfomkqyRtKHv3G2xfafuacvuX2/5GOVbzN8q7Zid6yVeX+zo0VY/Z9h/a3lc+3l9+7VoVQ6xut/0H0zzWc21/uxz06msTI9XZ/l3b95fHdUt556bKuxt3les+3rCfAds7y2PeZ/vC6dSBOa7b4/PyyP+Q9KyKAY9GJH1Z0qCKOxXPb9jmp8p/50naIelVKsamPiTpvHLdEhW/JV4p6ZqG1z7/XNJXJP12ufw7km4rl7dK+icVnZOVKoZNPbnOc1WMrPdiFUOl7pd0Trnu+5KWTvKatZKebDi+ERXDkF6m4o68eyT1l9tuUHHXoiS9rGEfn5D03nJ5u6QryuXfk3S0XP6ApD9taKPF3f5/5ZHnwakVtMOPnVopz5E/GsV4yhN+0/YmFUE9oCJsQ9JoRNwvSVGOdNdkVMQ1kn69XP68pL9qWHdbFKdwDpTjdZzsdZK+HMUgVbJ9q6QLJT3Y5Pjuiog3NRzf1nJxhaSzVYzUJxUBPDEuxtm2P6Fi8ohFKm5Pl6QLJP1GQ/2fLJfvl3SD7fnlcYw0qQl4HkGOujw9sWD7LEkfVNHz/kEZhC9SMabGTMeIaHz9sYblyX4atHvcXEvaHxFrJlm3VdJbImKv7StV9OwnvOCYI2Kn7ddLeqOkz9v+64j4XJvrRY/iHDk6YYmKYH+y7ClfWn79u5J+1vZ5kmR7se0+SU9JWjzFvu5RMXqeJF0u6e5p1LFT0ltsL7T9YhUTXNw1rSP5cQ9L6re9RpJsz7f9S+W6xZJGyx725Q2v+deT6lf52pdLOhwRn5H0WRXTgQGV0CNH7cpe6YMqzkkfUhFmiohnbG+Q9Le2T5P0I0kXS/qWpI/YHpH0Fyft7n0qTkF8SNKYpjGqZkQ8UP42MDHG9fUR0ey0yqn290z5R9Wryz/S9qmYOGO/pD+TtFvSoyrOy0/8YNos6Ubbm1VMdzdhraQP2f4/SUcl9epsQagBox8CQHKcWgGA5AhyAEiOIAeA5AhyAEiOIAeA5AhyAEiOIAeA5P4fSR1HcC8PCmsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define an experiment as flipping Ncoins\n",
    "# we will repeat the experiment Ntrials times\n",
    "# for each experiment, we record the fraction of heads(1's) observed\n",
    "\n",
    "def do_one_trial(pheads=0.5, Ncoins=10):\n",
    "    coin = [1,0] #H=1, T=0\n",
    "    total_flips = 1000 \n",
    "    toss_results = random.choices(coin,k=Ncoins,weights=[pheads,1-pheads])\n",
    "    fraction_successes = float(sum(toss_results)/Ncoins) # success = head\n",
    "    return fraction_successes\n",
    "\n",
    "def do_many_trials(pheads=0.5, Ntrials=1000, Ncoins=20):\n",
    "    resultarr = []\n",
    "    for i in range(0, Ntrials):\n",
    "        resultarr.append(do_one_trial(pheads=pheads, Ncoins=Ncoins))\n",
    "    return resultarr\n",
    "\n",
    "# looking to get the bounds of the rejection region for plotting\n",
    "def get_thresh_2tail(trial_data, significance=.1):\n",
    "    sorted_results = np.array(sorted(trial_data))\n",
    "    twotailedsig = significance/2.\n",
    "    # sum from left to right to find left most bound\n",
    "    for val in sorted_results:\n",
    "        integral = float(len(sorted_results[sorted_results<val]))/len(sorted_results)\n",
    "        if integral > twotailedsig:\n",
    "            lowerbound = val\n",
    "            break\n",
    "    # sum from right to left to find right most bound\n",
    "    for val in sorted_results[::-1]:\n",
    "        integral = float(len(sorted_results[sorted_results>val]))/len(sorted_results)\n",
    "        if integral > twotailedsig:\n",
    "            upperbound = val\n",
    "            break\n",
    "    return lowerbound, upperbound\n",
    "\n",
    "coin_experiment_data = do_many_trials(pheads=0.5, Ntrials=100, Ncoins=20)\n",
    "lowerbound, upperbound = get_thresh_2tail(coin_experiment_data, significance=0.1)\n",
    "\n",
    "# plot data with rejection regions\n",
    "hist, bins = np.histogram(coin_experiment_data, bins = np.linspace(0,1,11), density=True)\n",
    "hist = np.concatenate([hist,[0.]])\n",
    "plt.plot(bins, hist, drawstyle='steps-post')\n",
    "plt.xlabel('Fraction of Heads')\n",
    "plt.axvline(lowerbound, color='orange')\n",
    "plt.axvline(upperbound, color='orange')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "print(\"bounds are:\")\n",
    "print([lowerbound, upperbound])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到大部分时候还是50%概率，不过也有少于或多于50%的情况。现在的问题是我们怎么知道这是一个公平的硬币，我们怎么基于这些信息做决策，比如扔掉这个硬币。\n",
    "\n",
    "答案是我们可以设置一个我们做决定的区域，比如我们认为小于0.4或大于0.65的时候，硬币是不公平的。我们知道当我们这么做的时候，我们就有一定概率犯错误，所以我们要考虑犯错的后果。\n",
    "\n",
    "我们引入了损失函数的概念。损失函数（也称为成本函数或作为正值的效用函数）是我们创建的一个函数，它以最小化错误为目标，能考虑不同错误后果的差异。现在的目标是最小化平均损失：\n",
    "\n",
    "$$E[loss] = \\sum_k \\sum_j \\int_{\\mathbb{R_j}} L_{kj}p(x,C_k) dx$$\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "损失函数是为了将我们的目标与惩罚相匹配而创建的函数，以将我们的算法引导到有益的区域。例如，考虑以下损失函数：\n",
    "\n",
    "$$\\begin{matrix} & \\text{disease} & \\text{no disease} \\\\ \\text{disease} & 0 & 100 \\\\\\text{no disease} & 1 & 0 \\end{matrix}$$\n",
    "\n",
    "在这里，我们为正确响应分配零损失，为我们预测“没有疾病”但真实状态为“疾病”的情况分配 1 损失，最后，为错误预测“疾病”的情况分配 100 损失。这里的损失函数隐含地惩罚误报而不是漏报（见最后的混淆矩阵）。\n",
    "\n",
    "$$L(\\theta,a) = \n",
    "\\begin{cases}\n",
    "    0, \\text{for {(predict disease, actual disease),(predict no disease, actual no disease)}} \\\\\n",
    "    1, \\text{for predict no disease, actual disease} \\\\\n",
    "    100, \\text{for predict disease, actual no disease} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### Cross Entropy Loss\n",
    "\n",
    "任何是数据的经验分布和模型的概率分布之间的负对数似然的损失函数都是交叉熵损失。\n",
    "\n",
    "通常，当人们谈论交叉熵损失时，其含义是针对二元分类的。事实上，交叉熵损失，也称为对数损失，是二元分类任务的首选函数，它是伯努利试验的对数似然。\n",
    "\n",
    "$$-y_i ln(\\hat{\\theta}_i) - (1-y_i) ln (1-\\hat{\\theta}_i)$$\n",
    "\n",
    "交叉熵损失很好，因为它会严重惩罚非常错误的结果。在这种情况下，真正的标签是 \\\\(y_i\\\\) 的，而我们的预测是相关的概率（\\\\(\\hat{\\theta}_i\\\\) 的）。请注意，交叉熵损失没有“奖励”，当输出正确时，您能做的最好是 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalties for each prediction\n",
      "[[-0.05129319 -0.        ]\n",
      " [-1.89711932 -0.        ]\n",
      " [-0.         -1.38629396]\n",
      " [-0.         -0.04082189]]\n",
      "Total cross entropy loss is: 0.8438820897043499\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = np.array([[0.95,0.05],\n",
    "                        [0.15,0.85],\n",
    "                        [0.75,0.25],\n",
    "                        [0.04,0.96]]) #predictions are A,B,B,A\n",
    "targets = np.array([[1,0],\n",
    "                    [1,0],\n",
    "                    [0,1],\n",
    "                   [0,1]]) #correct classes are A,A,B,B\n",
    "\n",
    "epsilon=1e-10 # avoid log of zero\n",
    "predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "N = predictions.shape[0]\n",
    "x = targets * np.log(predictions+1e-7)\n",
    "print(\"Penalties for each prediction\")\n",
    "print(x)\n",
    "ce_loss = -np.sum(x)/N\n",
    "print (\"Total cross entropy loss is: \" + str(ce_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，这可以很容易地扩展到多个类别的情况。\n",
    "\n",
    "### Categorical cross entropy loss\n",
    "\n",
    "$$\\text{CE loss }= - \\sum_{i=1}^I \\sum_{k=1}^K \\mathbb{1}_{y_i = k}log p(y_i = k | x_i,\\theta)$$\n",
    "\n",
    "这看起来完全不同，但理解这些术语将很快帮助熟悉设置。CE 损失是分配给预测类别的概率之和。 \\\\(\\mathbb{1}\\\\) 是一个指示函数，用于确定哪个类是正确的类，而 \\\\(p(y_i = k | x_i,\\theta)\\\\) 是给定正确类的可能性数据。在两个类别的情况下，这简化为上面的交叉熵情况。\n",
    "\n",
    "### Other classification loss functions\n",
    "\n",
    "**Focal loss** 是一种修正损失，试图突出错误分类样本与正确分类样本的损失。\n",
    "\n",
    "$$FL = - \\sum_{i=1}^{C=2}(1-\\hat{\\theta}_i)^\\gamma t_i log(\\hat{\\theta}_i)$$ \n",
    "其中 \\\\((1-\\hat{\\theta}_i)^\\gamma\\\\) 调节损失。随着正确响应的分配概率趋向于 1，对损失的贡献会更快地变为 0。\n",
    "\n",
    "**Hinge loss** 是最大边距分类器。基本上，这种损失旨在使分类更加确定事件。正确作业的分数应比不正确作业的分数总和多出一定幅度。\n",
    "\n",
    "$$HL = \\sum_{j \\ne y_i} max(0,s_j-s_{y_i}+1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous response\n",
    "\n",
    "在连续响应中，拟合优度度量是距离的某个函数。\n",
    "\n",
    "#### Mean squared error \n",
    "\n",
    "均方误差 MSE 定义为 \n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "作为损失函数，它具有许多不错的特征，包括基于与预测和数学属性的距离进行惩罚，例如它是可微的并且总是有一个单一的解。 MSE 对异常值敏感，即它对异常值不稳健。\n",
    "\n",
    "#### Mean absolute error \n",
    "\n",
    "平均绝对误差 MAE 定义为 \n",
    "\n",
    "$$MAE = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y_i}|$$\n",
    "\n",
    "与 MSE 一样，MAE 返回误差均值，但需要计算工具来处理缺少微分的问题。 MAE 对 oulier 的敏感性低于 MSE，但是，不能保证唯一的解。\n",
    "\n",
    "#### L1 and L2 loss\n",
    "\n",
    "L1 损失是绝对差的总和，也称为最小绝对偏差 (LAD) 方法。 \\\\(L_1 = N \\ast MAE\\\\)。\n",
    "\n",
    "L2 损失是 $N \\ast MSE$ 也称为最小二乘误差或 LSE。\n",
    "\n",
    "#### Regularization \n",
    "\n",
    "正则化是一种通过向回归系数的总和添加惩罚项来防止过度拟合的技术。例如：\n",
    "\n",
    "$$argmin LSE + \\lambda \\sum_{i=1}^k|w_i|$$\n",
    "其中 \\\\(w_i\\\\) 是正在学习的权重。\n",
    "\n",
    "以上将是最小二乘法 (LASSO) 上的 L1 正则化。还有其他变化，包括 L2 正则化（岭回归），其中惩罚是平方权重，弹性网络（L1 和 L2 正则化）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "对于二元分类问题，我们有所谓的混淆矩阵。对于已知为 A 的类，我们预测的是 A 还是 B？该问题的结果可以总结在混淆矩阵中：\n",
    "\n",
    "\n",
    "|                     | Positive Class                    | Negative Class                       |\n",
    "|---------------------|-----------------------------------|--------------------------------------|\n",
    "| Positive Prediction | True Positive (TP)                | False Positive (FP) **Type I error** |\n",
    "| Negative Prediction | False Negative (FN) **Type II error** | True Negative (TN)              |\n",
    "\n",
    "各单元格产生 4 种可能的情况：TP、FP、FN、TN。与这些情况相关的是边界比例：\n",
    "\n",
    "真阳性率 (TPR)：TP/(TP+FN) -- **灵敏度**\n",
    "误报率 (FPR)：FP/(FP+TN)\n",
    "真负率 (TNR)：TN/(TN+FP) -- **特异性**\n",
    "假阴性率 (FNR)：FN/(FN+TP)\n",
    "\n",
    "顺便说一句，我们可以将这些映射到贝叶斯规则，其中\n",
    "\n",
    "P(A) = Probability of Positive Class (PC)  \n",
    "P(not A) = Probability of Negative Class (NC)  \n",
    "P(B) = Probability of Positive Prediction (PP)  \n",
    "P(not B) = Probability of Negative Prediction (NP)  \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{TPR*PC}{TPR*PC + FPR*NC}$$\n",
    "\n",
    "\n",
    "Precision also called the Positive Predictive Value.  \n",
    "$$precision = PPV = \\frac{TP}{TP+FP} = \\frac{TPR*PC}{PP}$$\n",
    "\n",
    "Recall is also known as the sensitivity.  \n",
    "$$recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$F_1 score = \\frac{precision*recall}{precision+recall}$$\n",
    "\n",
    "Type 1 error = false positive\n",
    "\n",
    "Type 2 error = false negative\n",
    "\n",
    "总之，面对不确定性做出决定是一种选择。有很多方法可以说明如何做出选择。简单地说，人们可以选择最有可能正确的选项。所有的选择都具有同等的后果吗？可能不是。为了允许这种有偏错误的观点，我们引入了损失函数的概念。损失函数是我们调整算法以找到代表我们错误的现实世界后果的最佳解决方案的机会。\n",
    "<hr style=\"border:2px solid blue\"> </hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
